{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 爬取巨牛汇所有人才信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "import requests\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpiderCow(object):\n",
    "    \n",
    "    def __init__(self, start_url, main_url, pages):\n",
    "        self.main_url = main_url\n",
    "        self.start_url = start_url\n",
    "        self.pages = pages\n",
    "        self.url_tail = []\n",
    "        \n",
    "    def _make_url(self):\n",
    "        for i in range(1, self.pages+1):\n",
    "            self.url_tail.append(\"{}?randomKey={}\".format(i, random.randint(1000, 9999)))\n",
    "    \n",
    "    def _request_url(self, url):\n",
    "        headers = {'User-Agent': UserAgent().random}\n",
    "        res = requests.get(url=self.start_url+url, headers=headers)\n",
    "        return res\n",
    "    \n",
    "    def run(self):\n",
    "        if not self.url_tail:\n",
    "            self._make_url()\n",
    "        for i in self.url_tail:\n",
    "            res = self._request_url(url=i)\n",
    "            self._analysis_html(res)\n",
    "            time.sleep(0.1)\n",
    "    \n",
    "    def _analysis_html(self, response):\n",
    "        res_html = response.text\n",
    "        res_soup = BeautifulSoup(res_html, \"lxml\")\n",
    "        usr_list = res_soup.find_all(name='div',attrs={\"class\":\"list\"})\n",
    "        for one_usr in usr_list[0]:\n",
    "            try:\n",
    "                usr_url = self.main_url + one_usr.a[\"href\"]    # 用户链接\n",
    "                usr_name = one_usr.h3.next_element             # 用户名称\n",
    "                usr_position = one_usr.h3.contents[2].text     # 用户岗位\n",
    "                usr_team = one_usr.h3.contents[3].text         # 个人或团队\n",
    "                usr_address = one_usr.h3.contents[4].text      # 用户地址\n",
    "                usr_seniority = one_usr.h3.contents[5].text    # 用户工龄\n",
    "                usr_skills = one_usr.p.text                    # 用户技能\n",
    "                usr_order = one_usr.contents[1].span.text      # 用户订单\n",
    "                usr_charges = one_usr.contents[1].em.text      # 用户订单\n",
    "                usr_product = one_usr.p.next_sibling.text      # 用户作品\n",
    "                print(f\"用户名称: {usr_name}\\t用户岗位: {usr_position}\\t用户地址: {usr_address}\\t个人或团队: {usr_team}\", end=\"\")\n",
    "                print(f\"\\t用户工龄: {usr_seniority}\\t用户订单: {usr_order}\\t收费标准: {usr_charges}\")\n",
    "                print(f\"用户技能: {usr_skills}\")\n",
    "                print(f\"用户作品: {usr_product}\")\n",
    "                print(f\"用户链接: {usr_url}\")\n",
    "                print(\"-\"*100)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            \n",
    "    def get_totle_pages(self):  # 获取总的页数\n",
    "        if not self.url_tail:\n",
    "            self._make_url()\n",
    "        res = self._request_url(self.url_tail[0])\n",
    "        res_html = res.text\n",
    "        res_soup = BeautifulSoup(res_html, \"lxml\")\n",
    "        total_pages = res_soup.find_all(name='span', attrs={\"class\":\"num\"})[0].contents[-1].text\n",
    "        print(total_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_url = \"https://www.juniuhui.com\"\n",
    "start_url = \"https://www.juniuhui.com/talentPool/talentList/index/\"\n",
    "pages = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spider_cow = SpiderCow(start_url=start_url, main_url=main_url, pages=pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spider_cow.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n"
     ]
    }
   ],
   "source": [
    "spider_cow.get_totle_pages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
