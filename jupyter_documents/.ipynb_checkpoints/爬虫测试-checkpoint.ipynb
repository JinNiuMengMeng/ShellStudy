{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "{'jb_result': -1, 'jb_msg': '用户名或密码错误'}\n",
      "{'Server': 'nginx', 'Date': 'Mon, 27 May 2019 07:00:03 GMT', 'Content-Type': 'text/html; charset=UTF-8', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Keep-Alive': 'timeout=2', 'Vary': 'Accept-Encoding', 'X-Powered-By': 'PHP/5.3.3', 'X-Robots-Tag': 'noindex', 'X-Content-Type-Options': 'nosniff', 'Expires': 'Wed, 11 Jan 1984 05:00:00 GMT', 'Cache-Control': 'no-cache, must-revalidate, max-age=0', 'Pragma': 'no-cache', 'X-Frame-Options': 'SAMEORIGIN', 'Content-Encoding': 'gzip'}\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    \"action\": \"user_login\",\n",
    "    \"user_login\": \"411317572@qq.com\",\n",
    "    \"user_pass\": \"Aa411317572\",\n",
    "    \"remember_me\": \"1\"\n",
    "}\n",
    "res = requests.post(\"http://www.jobbole.com/wp-admin/admin-ajax.php\", data=data)\n",
    "print(res.status_code)\n",
    "print(eval(res.content))\n",
    "print(res.headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "0\n",
      "{'Server': 'nginx', 'Date': 'Mon, 27 May 2019 06:39:12 GMT', 'Content-Type': 'text/html; charset=utf-8', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Keep-Alive': 'timeout=2', 'Vary': 'Accept-Encoding', 'X-Powered-By': 'PHP/5.3.3', 'Content-Encoding': 'gzip'}\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    \"action\": \"user_login\",\n",
    "    \"user_login\": \"411317572@qq.com\",\n",
    "    \"user_pass\": \"Aa411317572\",\n",
    "    \"remember_me\": \"1\"\n",
    "}\n",
    "res = requests.get(\"http://www.jobbole.com/wp-admin/admin-ajax.php\", data=data)\n",
    "print(res.status_code)\n",
    "print(eval(res.content))\n",
    "print(res.headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zaker 爬去"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://www.myzaker.com/channel/8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.text, 'html.parser', from_encoding='utf-8')\n",
    "result = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_a = soup.find_all('a')\n",
    "for i in tags:\n",
    "    result.append(urljoin(url, i[\"href\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://zkres.myzaker.com\n",
      "http://zkres1.myzaker.com\n",
      "http://zkres2.myzaker.com\n",
      "http://zkres.myzaker.com/static/zaker_web2/css/index.min.css?v=20170726\n",
      "http://zkres.myzaker.com/static/zaker_web2/css/normalize.min.css?v=20170122\n",
      "http://zkres.myzaker.com/static/zaker_web2/css/base.min.css?v=20170726\n",
      "http://zkres.myzaker.com/static/zaker_web2/css/hotSearch.min.css?v=20170726\n",
      "http://zkres.myzaker.com/static/zaker_web2/css/location.min.css?v=20171027\n"
     ]
    }
   ],
   "source": [
    "tags_link = soup.find_all('link')\n",
    "for i in tags_link:\n",
    "    result.append(urljoin(url, i[\"href\"]))\n",
    "    print(urljoin(url, i[\"href\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244\n"
     ]
    }
   ],
   "source": [
    "print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in result:\n",
    "#     print(i)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据抓取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`app`中的数据比`web`端数据更容易抓取, 反爬虫没有那么强, 大部分都是`http/https`协议, 返回的数据大部分为`json`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "困难部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 可能需要反编译, 分析出加密算法并抓取到信息\n",
    "2. 可能加固, 需要脱壳, 然后反编译, 分析出加密算法并抓取信息\n",
    "3. 需要破解通过各式各样的签名, 证书, 设备绑定等方法, 找到隐藏加密算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "掌握技术"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `python`爬虫经验\n",
    "2. `app`逆向\n",
    "3. `java`开发基础\n",
    "4. `app`脱壳\n",
    "5. `android`开发基础\n",
    "6. 破解加密算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于Docker打造的多任务抓取系统"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./pictures/2019-05-29_11-22.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GSLB 爬取站内所有url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "链接: http://www.cloudtopspeed.com/, 文件大小: 7762Byte, Referer: None\n",
      "{'Server': 'CACHE/CLOUDSPEED_1.0.4V', 'Date': 'Tue, 18 Jun 2019 02:31:33 GMT', 'Content-Type': 'text/html; charset=utf-8', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'X-Powered-By': 'Express', 'Set-Cookie': 'clientIp=117.28.112.195; Max-Age=2592000; Path=/; Expires=Thu, 18 Jul 2019 02:31:33 GMT, location=%7B%22cityName%22%3A%22%22%2C%22cityCode%22%3A%22%22%2C%22province%22%3A%22%22%7D; Max-Age=2592000; Path=/; Expires=Thu, 18 Jul 2019 02:31:33 GMT', 'ETag': '\"1e52-ow59uUu9J00BUY49VuPqGjb8thI\"', 'Vary': 'Accept-Encoding', 'Content-Encoding': 'gzip', 'Cache-Status': 'MISS'}\n",
      "链接: http://www.cloudtopspeed.com/products/data.html, 文件大小: 8326Byte, Referer: None\n",
      "{'Server': 'CACHE/CLOUDSPEED_1.0.4V', 'Date': 'Tue, 18 Jun 2019 02:31:36 GMT', 'Content-Type': 'text/html; charset=utf-8', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'X-Powered-By': 'Express', 'Set-Cookie': 'clientIp=117.28.112.195; Max-Age=2592000; Path=/; Expires=Thu, 18 Jul 2019 02:31:36 GMT, location=%7B%22cityName%22%3A%22%22%2C%22cityCode%22%3A%22%22%2C%22province%22%3A%22%22%7D; Max-Age=2592000; Path=/; Expires=Thu, 18 Jul 2019 02:31:36 GMT', 'ETag': '\"2086-GN7Nof6x2Cy5pRf+67Uq0Ng6kjA\"', 'Vary': 'Accept-Encoding', 'Content-Encoding': 'gzip', 'Cache-Status': 'MISS'}\n",
      "链接: http://www.cloudtopspeed.com/about.html, 文件大小: 5461Byte, Referer: None\n",
      "{'Server': 'CACHE/CLOUDSPEED_1.0.4V', 'Date': 'Tue, 18 Jun 2019 02:31:37 GMT', 'Content-Type': 'text/html; charset=utf-8', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'X-Powered-By': 'Express', 'Set-Cookie': 'clientIp=117.28.112.195; Max-Age=2592000; Path=/; Expires=Thu, 18 Jul 2019 02:31:37 GMT, location=%7B%22cityName%22%3A%22%22%2C%22cityCode%22%3A%22%22%2C%22province%22%3A%22%22%7D; Max-Age=2592000; Path=/; Expires=Thu, 18 Jul 2019 02:31:37 GMT', 'ETag': '\"1555-r5+IG3E8fDB5VzGj2Fyr5iAw4aU\"', 'Vary': 'Accept-Encoding', 'Content-Encoding': 'gzip', 'Cache-Status': 'MISS'}\n",
      "链接: http://www.cloudtopspeed.com/contact.html, 文件大小: 3556Byte, Referer: None\n",
      "{'Server': 'CACHE/CLOUDSPEED_1.0.4V', 'Date': 'Tue, 18 Jun 2019 02:31:39 GMT', 'Content-Type': 'text/html; charset=utf-8', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'X-Powered-By': 'Express', 'Set-Cookie': 'clientIp=117.28.112.195; Max-Age=2592000; Path=/; Expires=Thu, 18 Jul 2019 02:31:39 GMT, location=%7B%22cityName%22%3A%22%22%2C%22cityCode%22%3A%22%22%2C%22province%22%3A%22%22%7D; Max-Age=2592000; Path=/; Expires=Thu, 18 Jul 2019 02:31:39 GMT', 'ETag': '\"de4-l3Nq6HysqH/IITs1tnvZmkXFlcA\"', 'Vary': 'Accept-Encoding', 'Content-Encoding': 'gzip', 'Cache-Status': 'MISS'}\n",
      "链接: http://www.cloudtopspeed.com/products/ddos.html, 文件大小: 5500Byte, Referer: None\n",
      "{'Server': 'CACHE/CLOUDSPEED_1.0.4V', 'Date': 'Tue, 18 Jun 2019 02:31:41 GMT', 'Content-Type': 'text/html; charset=utf-8', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'X-Powered-By': 'Express', 'Set-Cookie': 'clientIp=117.28.112.195; Max-Age=2592000; Path=/; Expires=Thu, 18 Jul 2019 02:31:41 GMT, location=%7B%22cityName%22%3A%22%22%2C%22cityCode%22%3A%22%22%2C%22province%22%3A%22%22%7D; Max-Age=2592000; Path=/; Expires=Thu, 18 Jul 2019 02:31:41 GMT', 'ETag': '\"157c-c4DodMbw/uvIVgq5uO3ILssUdyg\"', 'Vary': 'Accept-Encoding', 'Content-Encoding': 'gzip', 'Cache-Status': 'MISS'}\n",
      "链接: http://www.cloudtopspeed.com/products/cloud.html, 文件大小: 5194Byte, Referer: None\n",
      "{'Server': 'CACHE/CLOUDSPEED_1.0.4V', 'Date': 'Tue, 18 Jun 2019 02:31:43 GMT', 'Content-Type': 'text/html; charset=utf-8', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'X-Powered-By': 'Express', 'Set-Cookie': 'clientIp=117.28.112.195; Max-Age=2592000; Path=/; Expires=Thu, 18 Jul 2019 02:31:43 GMT, location=%7B%22cityName%22%3A%22%22%2C%22cityCode%22%3A%22%22%2C%22province%22%3A%22%22%7D; Max-Age=2592000; Path=/; Expires=Thu, 18 Jul 2019 02:31:43 GMT', 'ETag': '\"144a-G3xMctE5fmk0FVm0lMSBpe5Tmno\"', 'Vary': 'Accept-Encoding', 'Content-Encoding': 'gzip', 'Cache-Status': 'MISS'}\n",
      "爬虫结束\n",
      "\n",
      "总的url数量: 7 , 如下:\n",
      "http://www.cloudtopspeed.com/\n",
      "http://www.cloudtopspeed.com/index.html\n",
      "http://www.cloudtopspeed.com/products/data.html\n",
      "http://www.cloudtopspeed.com/about.html\n",
      "http://www.cloudtopspeed.com/contact.html\n",
      "http://www.cloudtopspeed.com/products/ddos.html\n",
      "http://www.cloudtopspeed.com/products/cloud.html\n",
      "状态码非200数量: 1 , 如下:\n",
      "http://www.cloudtopspeed.com/chongyaq@cloudtopspeed.com\n",
      "请求超时url数量: 0 , 如下:\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "\n",
    "class Spider_Gslb(object):\n",
    "\n",
    "    def __init__(self, url, excluded_url):\n",
    "        self.url = url  # 入口\n",
    "        self.wait_url = set()  # 等待爬取的url\n",
    "        self.already_done = set(excluded_url)  # 已经爬取过的url\n",
    "        self.error_url = set()  # status code 不是200的url\n",
    "        self.time_out_url = set()  # 超时的url\n",
    "        self.page_size = 0\n",
    "        self.url_domain_name = None\n",
    "\n",
    "    def init_start_url_data(self):\n",
    "        self.url_domain_name = urlparse(self.url).netloc\n",
    "\n",
    "        res = requests.get(self.url)\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        tags = soup.find_all('a')\n",
    "        for i in tags:\n",
    "            self.wait_url.add(urljoin(self.url, i.get(\"href\")))\n",
    "\n",
    "    def web_page_size(self, res):\n",
    "        try:\n",
    "            page_size = res.headers[\"Content-Length\"]\n",
    "        except:\n",
    "            with open(\"temp.html\", \"w\") as fw:\n",
    "                fw.write(res.text)\n",
    "            page_size = os.path.getsize(\"temp.html\")\n",
    "        return page_size\n",
    "\n",
    "    def crawl(self):\n",
    "        while True:  # 循环不断从待爬取的集合中取出数据\n",
    "            try:\n",
    "                url_one = self.wait_url.pop()\n",
    "            except:  # 当集合为空时, 爬取完毕, 跳出循环\n",
    "                print(\"爬虫结束\\n\")\n",
    "                break\n",
    "            if url_one not in self.already_done:\n",
    "                if urlparse(url_one).netloc not in self.url_domain_name:  # 通过域名判断是否是第三方连接, 如果是不爬取\n",
    "                    self.already_done.add(url_one)\n",
    "                else:\n",
    "                    try:\n",
    "                        res = requests.get(url_one, timeout=5)\n",
    "                    except:\n",
    "                        self.time_out_url.add(url_one)\n",
    "                    else:\n",
    "                        if res.status_code != 200:\n",
    "                            self.error_url.add(url_one)\n",
    "                        else:\n",
    "                            print(\"链接: {}, 文件大小: {}Byte, Referer: {}\".format(res.url, self.web_page_size(res), res.headers.get(\"Referer\", None)))\n",
    "                            print(res.headers)\n",
    "                            soup = BeautifulSoup(res.text, 'html.parser')\n",
    "                            tags = soup.find_all('a')\n",
    "                            for i in tags:\n",
    "                                temp_url = urljoin(self.url, i.get(\"href\"))  # 判断第二子页是否存在爬取过的url\n",
    "                                if temp_url not in self.already_done:\n",
    "                                    self.wait_url.add(temp_url)\n",
    "                            self.already_done.add(url_one)\n",
    "            # print(\"剩余:{}, 已完成:{}, 超时:{}, 状态码非200:{}\".format(len(self.wait_url), len(self.already_done),\n",
    "            #                                                 len(self.time_out_url), len(self.error_url)))\n",
    "            time.sleep(0.5)\n",
    "\n",
    "    def print_data(self):\n",
    "        print(\"总的url数量:\", len(self.already_done), \", 如下:\")\n",
    "        for i in self.already_done:\n",
    "            print(i)\n",
    "        print(\"状态码非200数量:\", len(self.error_url), \", 如下:\")\n",
    "        for i in self.error_url:\n",
    "            print(i)\n",
    "        print(\"请求超时url数量:\", len(self.time_out_url), \", 如下:\")\n",
    "        for i in self.time_out_url:\n",
    "            print(i)\n",
    "\n",
    "    # 程序启动入口\n",
    "    def start_app(self):\n",
    "        self.init_start_url_data()\n",
    "        self.crawl()\n",
    "        self.print_data()\n",
    "\n",
    "\n",
    "start_url = \"http://www.cloudtopspeed.com/\"\n",
    "already_done = [start_url + \"index.html\"]\n",
    "Spider_Gslb(url=start_url, excluded_url=already_done).start_app()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_url = \"http://103.118.36.248\"    \n",
    "already_done = [start_url+\"/\", start_url+\"/auth/login\", start_url+\"/auth/register\"]\n",
    "Spider_Gslb(url=start_url, excluded_url=already_done).start_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.head(\"http://www.cloudtopspeed.com/\", timeout=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'requests.structures.CaseInsensitiveDict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(res.headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172\n"
     ]
    }
   ],
   "source": [
    "print(res.headers[\"Content-Length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
